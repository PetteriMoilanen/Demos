{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZPkWVQNZERR"
      },
      "source": [
        "# Introducing Groq\n",
        "(The following summary generated by Gemini 2.0 Flash with the prompt *\"Give me a shortish rundown of the Groq LLM platform.\"*)\n",
        "\n",
        "\n",
        ">[Groq](https://groq.com/) is a company that has developed a platform specifically designed for running Large Language Models (LLMs) with a focus on real-time performance. Here's a rundown of their key aspects:\n",
        "\n",
        ">Core Technology:\n",
        "\n",
        ">>**Language Processing Unit (LPU):** Groq's unique hardware, the LPU, is a specialized chip architecture designed to accelerate LLM inference. It prioritizes sequential processing, which is crucial for generating text word by word, unlike traditional GPUs that excel at parallel computations.\n",
        "  \n",
        ">>**Software-First Approach:** Groq emphasizes software optimization, allowing their compiler to manage hardware execution and data flow. This simplifies development and speeds up deployment.\n",
        "  \n",
        ">Key Advantages:\n",
        "\n",
        ">>**Speed:** Groq claims to offer the fastest inference performance in the industry, measured in tokens per second per user. This translates to quicker response times and a more interactive user experience.\n",
        "\n",
        ">>**Efficiency:** Their platform is designed to be energy-efficient, which is important for cost savings and environmental considerations.\n",
        "\n",
        ">>**Scalability:** Groq's architecture is built to maintain high performance even as the scale of LLM deployments increases.\n",
        "\n",
        ">Use Cases:\n",
        "\n",
        ">Groq's platform is well-suited for applications that demand real-time LLM interactions, such as:\n",
        "\n",
        ">>**Chatbots and Virtual Assistants:** Where quick responses are essential for a natural conversation.\n",
        "  \n",
        ">>**Real-time Video Analytics:** For processing video feeds and making immediate decisions.\n",
        "\n",
        ">>**Predictive Analytics:** Where large datasets need to be analyzed rapidly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhDKa6u9cipj"
      },
      "source": [
        "## 1 A simple example\n",
        "\n",
        "(Slightly modified version of https://console.groq.com/docs/quickstart).\n",
        "\n",
        "I'm using Colab secrets to API keys. See for instance [Gemini API: Authentication Quickstart](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STu9ubQbZBmr",
        "outputId": "08b2e542-662e-4c02-e6d7-f828d9951bd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m112.6/121.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m112.6/121.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3d60i709ZDOM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "\"\"\" If you're running this outside of Colab...\n",
        "\n",
        "import getpass\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOJG9u9TgQkG",
        "outputId": "20591db0-0d1f-4ce1-e77b-a9ab791e0254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fast language models are crucial in today's technology landscape, and their importance can be seen in various aspects. Here are some key reasons why fast language models are essential:\n",
            "\n",
            "1. **Efficient Processing**: Fast language models can process and analyze large amounts of text data quickly, making them ideal for applications such as text classification, sentiment analysis, and language translation. This efficiency enables businesses and organizations to gain insights and make decisions faster.\n",
            "2. **Real-time Applications**: Fast language models are necessary for real-time applications like chatbots, virtual assistants, and language translation software. These models can quickly respond to user queries, providing a seamless and interactive experience.\n",
            "3. **Scalability**: As the amount of text data grows, fast language models can handle the increased volume and velocity, making them scalable and able to support large-scale applications.\n",
            "4. **Improved User Experience**: Fast language models can improve user experience by providing quick and accurate responses to queries, which is particularly important in applications like customer service chatbots, where timely responses are critical.\n",
            "5. **Competitive Advantage**: Companies that adopt fast language models can gain a competitive advantage by quickly analyzing and responding to customer feedback, improving their products and services, and providing better customer support.\n",
            "6. **Research and Development**: Fast language models can accelerate research and development in areas like natural language processing (NLP), machine learning, and artificial intelligence (AI). By quickly processing and analyzing large datasets, researchers can focus on higher-level tasks and make new discoveries.\n",
            "7. **Resource Optimization**: Fast language models can optimize resource utilization, reducing the need for large computational resources and minimizing energy consumption. This is particularly important for applications running on edge devices or in resource-constrained environments.\n",
            "8. **Enhanced Security**: Fast language models can help detect and respond to security threats in real-time, such as identifying malicious text or detecting phishing attacks.\n",
            "9. **Accessibility**: Fast language models can improve accessibility for people with disabilities, such as those who rely on text-to-speech systems or language translation software.\n",
            "10. **Future-Proofing**: As language models continue to evolve, fast language models will be better equipped to handle future advancements, such as more complex NLP tasks, larger datasets, and increased demands for real-time processing.\n",
            "\n",
            "To achieve fast language models, researchers and developers can employ various techniques, such as:\n",
            "\n",
            "* Model pruning and compression\n",
            "* Knowledge distillation\n",
            "* Quantization and integerization\n",
            "* Parallelization and distributed computing\n",
            "* Specialized hardware acceleration (e.g., GPUs, TPUs, and FPGAs)\n",
            "\n",
            "By prioritizing the development of fast language models, we can unlock new possibilities for NLP applications, improve user experiences, and drive innovation in various industries.\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnbRKGqOgiOB"
      },
      "source": [
        "## 2 Using Groq with LangChain\n",
        "\n",
        "A modified excerpt from https://python.langchain.com/docs/tutorials/llm_chain/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uucAugAPgcKM",
        "outputId": "a63cc66f-5db7-4ed8-e69c-fc39361aab95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/413.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/413.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.2/413.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \"langchain[groq]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_BP5cLCwispi"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nuZx52IFiz1c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(\"hi!\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdS6XS7WjG6j",
        "outputId": "bd48a66a-727d-4192-c38a-82f093b9b3d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 24, 'total_tokens': 28, 'completion_time': 0.003333333, 'prompt_time': 0.005088586, 'queue_time': 0.09063891299999999, 'total_time': 0.008421919}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-e41cafcf-add0-4a82-988c-4846709b2f9e-0', usage_metadata={'input_tokens': 24, 'output_tokens': 4, 'total_tokens': 28})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0MD-qO_zjB__",
        "outputId": "53cae1f4-9cda-4fd6-f9fc-fbf59383d68c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ciao!'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbt5GnOIj1Du"
      },
      "source": [
        "## 3 Resources\n",
        "\n",
        "- [Groq Cloud Console](https://console.groq.com/playground) has a lot of stuff: a playground for trying out prompts and models, API keys generation, [documentation](https://console.groq.com/docs/overview), etc.\n",
        "- [Examples](https://console.groq.com/docs/examples) part of the documentation has a bunch of \"simple python applications you can fork and run in [Replit](https://replit.com/) to get you started building with Groq\".\n",
        "- **Replit** is \"an online integrated development environment (IDE) that provides a collaborative, browser-based platform for coding, learning, and building software\" (Gemini again). See also [Replit on Wikipedia](https://en.wikipedia.org/wiki/Replit#:~:text=Features,a%20variety%20of%20programming%20languages.).\n",
        "- [Groq API Cookbook](https://github.com/groq/groq-api-cookbook) is a GitHub repo with \"a collection of example code and guides for Groq API \".\n",
        "- There's also a [developer community on Discord](https://discord.com/invite/groq).\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
