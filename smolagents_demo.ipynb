{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Hugging Face's Smolagents\n",
    "\n",
    "A stand-alone notebook, basicly a stripped-down copy of the article [Hugging Face's Smolagents: A Guide With Examples](https://app.datacamp.com/learn/tutorials/smolagents) in Datacamp. \n",
    "\n",
    "## What and why it is\n",
    "\n",
    "As described by Hugging Face’s [announcement blog](https://huggingface.co/blog/smolagents), **smolagents** is *“a very simple library that unlocks agentic capabilities for language models.”* \n",
    "\n",
    "At their heart, agents are powered by LLMs to dynamically solve a task by observing their environments, making plans, and executing those plans given their toolbox. Building these agents, while not impossible, requires you to write from scratch many components. These components ensure that the agents function properly without burning through your API credit and execution time. Agentic frameworks make this easier so you don’t have to reinvent the wheels.\n",
    "\n",
    "AI agent frameworks are often criticized with two points:\n",
    "\n",
    "- They build too many layers of abstraction, making them rigid and challenging to debug.\n",
    "- They focus on “workflows” rather than building agents that can dynamically collaborate on their own.\n",
    "\n",
    "On the other hand, smolagents has (we are told) qualities that make it very promising for these agentic applications:\n",
    "\n",
    "- The framework’s abstractions are kept at a minimum.\n",
    "- While most frameworks have the agents define their actions in JSON/text format, smolagents’ main approach is **Code Agents** in which actions are written as Python code snippets (this is different from agents that write code).\n",
    "- Being a Hugging Face framework, smolagents integrates well with the Hub and the Transformers library. You can use many models from the hub (some of them you can only use as a Pro user), and you can also work with proprietary models from OpenAI, Anthropic, etc.\n",
    "- You can easily utilize the already-provided tools, or define your custom tools with minimum effort, almost as simple as writing a Python function.\n",
    "\n",
    "## Building a Demo Project With Smolagents\n",
    "\n",
    "In this section, we will build a simple demo with smolagents. Our application will have an agent get the most upvoted paper on the Hugging Face [Daily Papers page](https://huggingface.co/papers). We build our custom tools for the agent and see it work in action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU smolagents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building custom tools\n",
    "\n",
    "While the framework provides built-in tools (e.g. DuckDuckGoSearchTool), building your custom tools is just as straightforward. We'll build four tools, each for a particular purpose:\n",
    "\n",
    "- Getting the title of the top daily paper.\n",
    "- Getting the ID of the paper using its title.\n",
    "- Downloading the paper from arXiv with the ID.\n",
    "- Reading the downloaded PDF file.\n",
    "\n",
    "It’s important to ensure agents clearly understand which tool to use and how to use it. To achieve this, be as explicit as possible when defining these tools:\n",
    "\n",
    "- Choose an informative name for the function.\n",
    "- The inputs and outputs of the function should have type hints.\n",
    "- A description of the tool's purpose must be included. This serves as a manual to the agent.\n",
    "\n",
    "#### Tool1: Get the title of the top daily paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import tool \n",
    "# import packages that are used in our tools\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def get_hugging_face_top_daily_paper() -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the most upvoted paper on Hugging Face daily papers.\n",
    "    It returns the title of the paper\n",
    "    \"\"\"\n",
    "    try:\n",
    "      url = \"<https://huggingface.co/papers>\"\n",
    "      response = requests.get(url)\n",
    "      response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "      soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "      # Extract the title element from the JSON-like data in the \"data-props\" attribute\n",
    "      containers = soup.find_all('div', class_='SVELTE_HYDRATER contents')\n",
    "      top_paper = \"\"\n",
    "\n",
    "      for container in containers:\n",
    "          data_props = container.get('data-props', '')\n",
    "          if data_props:\n",
    "              try:\n",
    "                  # Parse the JSON-like string\n",
    "                  json_data = json.loads(data_props.replace('&quot;', '\"'))\n",
    "                  if 'dailyPapers' in json_data:\n",
    "                      top_paper = json_data['dailyPapers'][0]['title']\n",
    "              except json.JSONDecodeError:\n",
    "                  continue\n",
    "\n",
    "      return top_paper\n",
    "    except requests.exceptions.RequestException as e:\n",
    "      print(f\"Error occurred while fetching the HTML: {e}\")\n",
    "      return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool 2: Get the paper ID by its title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "@tool\n",
    "def get_paper_id_by_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the arxiv paper id by its title.\n",
    "    It returns the title of the paper\n",
    "\n",
    "    Args:\n",
    "        title: The paper title for which to get the id.\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    papers = api.list_papers(query=title)\n",
    "    if papers:\n",
    "        paper = next(iter(papers))\n",
    "        return paper.id\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool 3: Download the paper from arXiv with the ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "@tool\n",
    "def download_paper_by_id(paper_id: str) -> None:\n",
    "    \"\"\"\n",
    "    This tool gets the id of a paper and downloads it from arxiv. It saves the paper locally \n",
    "    in the current directory as \"paper.pdf\".\n",
    "\n",
    "    Args:\n",
    "        paper_id: The id of the paper to download.\n",
    "    \"\"\"\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "    paper.download_pdf(filename=\"paper.pdf\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool 4: Read a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "@tool\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    This function reads the first three pages of a PDF file and returns its content as a string.\n",
    "    Args:\n",
    "        file_path: The path to the PDF file.\n",
    "        pages_to_read:\n",
    "    Returns:\n",
    "        A string containing the content of the PDF file.\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "    reader = PdfReader('paper.pdf')\n",
    "    print(len(reader.pages))\n",
    "    pages = reader.pages[:3]\n",
    "    for page in pages:\n",
    "        content += page.extract_text()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A sidebar (not in the original paper): Qwen models\n",
    "\n",
    "Qwen is  a series of large language models (LLMs) and large multimodal models (LMMs) that are designed to perform various tasks, including natural language understanding, mathematical problem-solving, and coding.\n",
    "\n",
    "Some of the key features and capabilities of Qwen include:\n",
    "\n",
    "- Strong performance: Qwen models have demonstrated competitive performance on various benchmarks, often outperforming similar-sized models.\n",
    "- Multimodal capabilities: Some Qwen models are multimodal, meaning they can process and understand both text and images.\n",
    "- Open source: Alibaba Cloud has made some of the Qwen models available open source, encouraging further research and development.\n",
    "\n",
    "Resources:\n",
    "- About Qwen: https://qwenlm.github.io/about/\n",
    "- GitHub: This is where you'll find the code, documentation, and updates for the Qwen models: https://github.com/QwenLM/Qwen\n",
    "- Hugging Face: You can explore the Qwen models, download them, and even try them out in your browser: https://huggingface.co/QwenLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "With our tools set up, we can now initialize and run our agent. We use the Qwen2.5-Coder-32B-Instruct model, which is free to use. The tools an agent needs can be passed while defining the agent. You can see the process of defining and running an agent requires minimum code.\n",
    "\n",
    "As the agent operates, it outputs its process step by step. This allows us to see how it defines its actions in code while utilizing the custom tools we’ve provided.\n",
    "\n",
    "Before running the agent, we must set the API key. We're using GitHub Secrets for storing the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"HF_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Summarize today's top paper on Hugging Face daily papers by reading it.</span>                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mSummarize today's top paper on Hugging Face daily papers by reading it.\u001b[0m                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(ReadTimeoutError(</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">timeout=120)\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">), </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'(Request ID: 1373c34d-f362-4181-9c2f-947f850b84b8)'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[1;31m(\u001b[0m\u001b[1;31mReadTimeoutError\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m\"HTTPSConnectionPool\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mhost\u001b[0m\u001b[1;31m='api-inference.huggingface.co', \u001b[0m\u001b[1;31mport\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;31m443\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m: Read timed out. \u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mread \u001b[0m\n",
       "\u001b[1;31mtimeout\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;31m120\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m\"\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m, \u001b[0m\u001b[1;31m'\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mRequest ID: 1373c34d-f362-4181-9c2f-947f850b84b8\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m'\u001b[0m\u001b[1;31m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 120.08 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 0: Duration 120.08 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">try</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">:</span><span style=\"background-color: #272822\">                                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    top_paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_hugging_face_top_daily_paper()</span><span style=\"background-color: #272822\">                                                       </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Top paper title:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, top_paper_title)</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">except</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">Exception</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> e:</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Failed to get top paper title from Hugging Face:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, e)</span><span style=\"background-color: #272822\">                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Fallback: search for the top Hugging Face paper using web search</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    top_paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"most upvoted Hugging Face daily paper\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Top paper title from web search:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, top_paper_title)</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mtry\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_hugging_face_top_daily_paper\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTop paper title:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mexcept\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mException\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mFailed to get top paper title from Hugging Face:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Fallback: search for the top Hugging Face paper using web search\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mmost upvoted Hugging Face daily paper\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTop paper title from web search:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while fetching the HTML: No connection adapters were found for '<https://huggingface.co/papers>'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Top paper title: None\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Top paper title: None\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 9.21 seconds| Input tokens: 2,511 | Output tokens: 175]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 9.21 seconds| Input tokens: 2,511 | Output tokens: 175]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">try</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">:</span><span style=\"background-color: #272822\">                                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    top_paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"most upvoted Hugging Face daily paper\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Top paper title from web search:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, top_paper_title)</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">except</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">Exception</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> e:</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Failed to get top paper title from web search:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, e)</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mtry\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mmost upvoted Hugging Face daily paper\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTop paper title from web search:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mexcept\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mException\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mFailed to get top paper title from web search:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Top paper title from web search: ## Search Results\n",
       "\n",
       "[Daily Papers - Hugging Face](https://huggingface.co/papers)\n",
       "Your daily dose of AI research from AK. Get trending papers in your email inbox once a day! Get trending papers in \n",
       "your email inbox!\n",
       "\n",
       "[Exploring the Daily Papers Page on Hugging Face](https://huggingface.co/blog/daily-papers)\n",
       "In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers\n",
       "and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its \n",
       "launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the\n",
       "past year, more than 3,700 papers have been ...\n",
       "\n",
       "[Top 9 Upvoted Papers on Hugging Face - Analytics \n",
       "Vidhya](https://www.analyticsvidhya.com/blog/2025/01/top-upvoted-papers-on-hugging-face/)\n",
       "It focuses on the most upvoted papers. These papers resonate well with the Hugging Face community. The selection \n",
       "celebrates the work of researchers. It also promotes knowledge sharing among AI practitioners. The dynamic \n",
       "engagement on Hugging Face reflects current trends. This helps readers stay informed about cutting-edge AI \n",
       "research.\n",
       "\n",
       "[High 9 Upvoted Papers on Hugging Face in 2025 - \n",
       "codesanitize](https://codesanitize.com/high-9-upvoted-papers-on-hugging-face-in-2025/)\n",
       "This text on \"High Upvoted Papers on HuggingFace\" highlights influential analysis. It focuses on essentially the \n",
       "most upvoted papers. These papers resonate properly with the Hugging Face group. The choice celebrates the work of \n",
       "researchers. It additionally promotes information sharing amongst AI practitioners.\n",
       "\n",
       "[Daily Papers - Hugging Face](https://huggingface.co/papers?search=true)\n",
       "Your daily dose of AI research from AK. Hugging Face. Models; Datasets; Spaces; Posts; Docs; Enterprise; Pricing \n",
       "Log In Sign Up new. Get trending papers in your email inbox once a day! Get trending papers in your email inbox! \n",
       "... by AK and the research community. Search by arxiv id or title Submit a paper. Jan 24. Submitted by \n",
       "alsu-sagirova. 53.\n",
       "\n",
       "[blog/daily-papers.md at main · huggingface/blog - \n",
       "GitHub](https://github.com/huggingface/blog/blob/main/daily-papers.md)\n",
       "In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers\n",
       "and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its \n",
       "launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the\n",
       "past year, more than 3,700 papers have been ...\n",
       "\n",
       "[HuggingFace Paper Explorer](https://huggingface-paper-explorer.vercel.app/)\n",
       "All paper data and content belong to their respective owners and the HuggingFace community. Showing 61 unique \n",
       "papers. Baichuan-Omni Technical Report. Submitted by kenshinn. 69. 6. View on HuggingFace arXiv PDF. WALL-E: World \n",
       "Alignment by Rule Learning Improves World Model-based LLM Agents ...\n",
       "\n",
       "[Top 9 Upvoted Papers on Hugging Face - thenote.app](https://thenote.app/note?id=CdXz5zHNQW_QLTiJ8D1am)\n",
       "The field of artificial intelligence is changing rapidly. Therefore, to keep abreast of the most recent research, \n",
       "reviewing Papers on Hugging Face is essential. Hugging Face has created a unique space where researchers not only \n",
       "share their work but can also engage with the community by upvoting, commenting, and discussing with others.\n",
       "\n",
       "[[N] Daily Papers by Hugging Face : r/MachineLearning - \n",
       "Reddit](https://www.reddit.com/r/MachineLearning/comments/13lh43m/n_daily_papers_by_hugging_face/)\n",
       "According to Hugging Face's CTO, Julien Chaumond, \"AK has posted ~17,000 tweets daily, tirelessly curating the new \n",
       "research drops from Arxiv. This is our own \"AK feed\" directly on HF, where each paper is linked to its related \n",
       "models/datasets, and Spaces\". Another source to get your daily dose of AI research 🤗\n",
       "\n",
       "[Daily AI Papers | All credits go to HuggingFace's Daily AI papers ...](https://gabrielchua.me/daily-ai-papers/)\n",
       "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. Daily \n",
       "AI Papers. ... The paper presents a noise rescaling mechanism, ensuring consistent behavior across a diverse range \n",
       "of images, and demonstrates its effectiveness through both qualitative and quantitative analysis on various editing\n",
       "...\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Top paper title from web search: ## Search Results\n",
       "\n",
       "[Daily Papers - Hugging Face](https://huggingface.co/papers)\n",
       "Your daily dose of AI research from AK. Get trending papers in your email inbox once a day! Get trending papers in \n",
       "your email inbox!\n",
       "\n",
       "[Exploring the Daily Papers Page on Hugging Face](https://huggingface.co/blog/daily-papers)\n",
       "In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers\n",
       "and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its \n",
       "launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the\n",
       "past year, more than 3,700 papers have been ...\n",
       "\n",
       "[Top 9 Upvoted Papers on Hugging Face - Analytics \n",
       "Vidhya](https://www.analyticsvidhya.com/blog/2025/01/top-upvoted-papers-on-hugging-face/)\n",
       "It focuses on the most upvoted papers. These papers resonate well with the Hugging Face community. The selection \n",
       "celebrates the work of researchers. It also promotes knowledge sharing among AI practitioners. The dynamic \n",
       "engagement on Hugging Face reflects current trends. This helps readers stay informed about cutting-edge AI \n",
       "research.\n",
       "\n",
       "[High 9 Upvoted Papers on Hugging Face in 2025 - \n",
       "codesanitize](https://codesanitize.com/high-9-upvoted-papers-on-hugging-face-in-2025/)\n",
       "This text on \"High Upvoted Papers on HuggingFace\" highlights influential analysis. It focuses on essentially the \n",
       "most upvoted papers. These papers resonate properly with the Hugging Face group. The choice celebrates the work of \n",
       "researchers. It additionally promotes information sharing amongst AI practitioners.\n",
       "\n",
       "[Daily Papers - Hugging Face](https://huggingface.co/papers?search=true)\n",
       "Your daily dose of AI research from AK. Hugging Face. Models; Datasets; Spaces; Posts; Docs; Enterprise; Pricing \n",
       "Log In Sign Up new. Get trending papers in your email inbox once a day! Get trending papers in your email inbox! \n",
       "... by AK and the research community. Search by arxiv id or title Submit a paper. Jan 24. Submitted by \n",
       "alsu-sagirova. 53.\n",
       "\n",
       "[blog/daily-papers.md at main · huggingface/blog - \n",
       "GitHub](https://github.com/huggingface/blog/blob/main/daily-papers.md)\n",
       "In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers\n",
       "and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its \n",
       "launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the\n",
       "past year, more than 3,700 papers have been ...\n",
       "\n",
       "[HuggingFace Paper Explorer](https://huggingface-paper-explorer.vercel.app/)\n",
       "All paper data and content belong to their respective owners and the HuggingFace community. Showing 61 unique \n",
       "papers. Baichuan-Omni Technical Report. Submitted by kenshinn. 69. 6. View on HuggingFace arXiv PDF. WALL-E: World \n",
       "Alignment by Rule Learning Improves World Model-based LLM Agents ...\n",
       "\n",
       "[Top 9 Upvoted Papers on Hugging Face - thenote.app](https://thenote.app/note?id=CdXz5zHNQW_QLTiJ8D1am)\n",
       "The field of artificial intelligence is changing rapidly. Therefore, to keep abreast of the most recent research, \n",
       "reviewing Papers on Hugging Face is essential. Hugging Face has created a unique space where researchers not only \n",
       "share their work but can also engage with the community by upvoting, commenting, and discussing with others.\n",
       "\n",
       "[[N] Daily Papers by Hugging Face : r/MachineLearning - \n",
       "Reddit](https://www.reddit.com/r/MachineLearning/comments/13lh43m/n_daily_papers_by_hugging_face/)\n",
       "According to Hugging Face's CTO, Julien Chaumond, \"AK has posted ~17,000 tweets daily, tirelessly curating the new \n",
       "research drops from Arxiv. This is our own \"AK feed\" directly on HF, where each paper is linked to its related \n",
       "models/datasets, and Spaces\". Another source to get your daily dose of AI research 🤗\n",
       "\n",
       "[Daily AI Papers | All credits go to HuggingFace's Daily AI papers ...](https://gabrielchua.me/daily-ai-papers/)\n",
       "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. Daily \n",
       "AI Papers. ... The paper presents a noise rescaling mechanism, ensuring consistent behavior across a diverse range \n",
       "of images, and demonstrates its effectiveness through both qualitative and quantitative analysis on various editing\n",
       "...\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 10.08 seconds| Input tokens: 5,372 | Output tokens: 355]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 10.08 seconds| Input tokens: 5,372 | Output tokens: 355]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m4\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">try</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">:</span><span style=\"background-color: #272822\">                                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    top_paper_title_analytics </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">visit_webpage(url</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"https://www.analyticsvidhya.com/blog/2025/01/top-upvoted-papers-on-hugging-face/\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Content from Analytics Vidhya:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, top_paper_title_analytics)</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">except</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">Exception</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> e:</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Failed to visit Analytics Vidhya webpage:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, e)</span><span style=\"background-color: #272822\">                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">try</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">:</span><span style=\"background-color: #272822\">                                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    top_paper_title_thenote </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> visit_webpage(url</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"https://thenote.app/note?id=CdXz5zHNQW_QLTiJ8D1am\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Content from thenote.app:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, top_paper_title_thenote)</span><span style=\"background-color: #272822\">                                                </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">except</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">Exception</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> e:</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Failed to visit thenote.app webpage:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, e)</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mtry\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title_analytics\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mvisit_webpage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34murl\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhttps://www.analyticsvidhya.com/blog/2025/01/top-upvoted-papers-on-hugging-face/\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mContent from Analytics Vidhya:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title_analytics\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mexcept\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mException\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mFailed to visit Analytics Vidhya webpage:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mtry\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title_thenote\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mvisit_webpage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34murl\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhttps://thenote.app/note?id=CdXz5zHNQW_QLTiJ8D1am\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mContent from thenote.app:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title_thenote\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mexcept\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mException\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mFailed to visit thenote.app webpage:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Content from Analytics Vidhya: Top 9 Upvoted Papers on Hugging Face in 2025\n",
       "\n",
       " \n",
       "\n",
       "[![Analytics \n",
       "Vidhya](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/icon/av-logo-svg.svg)](https://www.analy\n",
       "ticsvidhya.com/blog/)\n",
       "\n",
       "* \n",
       "[DeepSeek](https://courses.analyticsvidhya.com/courses/getting-started-with-deepseek?utm_source=blog&amp;utm_medium=nav\n",
       "bar&amp;utm_campaign=deepseek-free-course&amp;utm_term=29-Jan-2024)\n",
       "* [Learning Paths](https://www.analyticsvidhya.com/blog/category/learning-path/?ref=navbar)\n",
       "* [GenAI Pinnacle Plus Program](https://www.analyticsvidhya.com/pinnacleplus/?ref=blognavbar)\n",
       "  New\n",
       "* [Agentic AI Pioneer Program](https://www.analyticsvidhya.com/agenticaipioneer/?ref=blognavbar)\n",
       "\n",
       "* Login\n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  + Switch Mode\n",
       "  + [Logout](#)\n",
       "\n",
       "[Interview \n",
       "Prep](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=category)[Career](https://www.analytic\n",
       "svidhya.com/blog/category/career/?ref=category)[GenAI](https://www.analyticsvidhya.com/blog/category/generative-ai/\n",
       "?ref=category)[Prompt \n",
       "Engg](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=category)[ChatGPT](https://www.analytic\n",
       "svidhya.com/blog/category/chatgpt/?ref=category)[LLM](https://www.analyticsvidhya.com/blog/category/llms/?ref=categ\n",
       "ory)[Langchain](https://www.analyticsvidhya.com/blog/category/langchain/?ref=category)[RAG](https://www.analyticsvi\n",
       "dhya.com/blog/category/rag/?ref=category)[AI \n",
       "Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/?ref=category)[Machine \n",
       "Learning](https://www.analyticsvidhya.com/blog/category/machine-learning/?ref=category)[Deep \n",
       "Learning](https://www.analyticsvidhya.com/blog/category/deep-learning/?ref=category)[GenAI \n",
       "Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=category)[LLMOps](https://www.analyticsvidhya.co\n",
       "m/blog/category/llmops/?ref=category)[Python](https://www.analyticsvidhya.com/blog/category/python/?ref=category)[N\n",
       "LP](https://www.analyticsvidhya.com/blog/category/nlp/?ref=category)[SQL](https://www.analyticsvidhya.com/blog/cate\n",
       "gory/sql/?ref=category)[AIML Projects](https://www.analyticsvidhya.com/blog/category/project/?ref=category) \n",
       "\n",
       "![](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/av_loader.gif)\n",
       "\n",
       "1. [Home](https://www.analyticsvidhya.com/blog/)\n",
       "2. [Research Paper](https://www.analyticsvidhya.com/blog/category/research-paper/)\n",
       "3. Top 9 Upvoted Papers on Hugging Face\n",
       "\n",
       "Top 9 Upvoted Papers on Hugging Face\n",
       "====================================\n",
       "\n",
       "[![Yana \n",
       "Khare](https://av-eks-lekhak.s3.amazonaws.com/media/lekhak-profile-images/converted_image_gDtaWsc.webp)](https://ww\n",
       "w.analyticsvidhya.com/blog/author/yana_khare/)\n",
       "\n",
       "[Yana Khare](https://www.analyticsvidhya.com/blog/author/yana_khare/) \n",
       " Last Updated :\n",
       "07 Jan, 2025\n",
       "\n",
       "  8  min read\n",
       "\n",
       "The field of [artificial \n",
       "intelligence](https://www.analyticsvidhya.com/blog/2021/09/introduction-to-artificial-intelligence-for-beginners/) \n",
       "is changing rapidly. Therefore, to keep abreast of the most recent research, reviewing Papers on [Hugging \n",
       "Face](https://www.analyticsvidhya.com/blog/2023/10/hugging-face-fine-tuning-tutorial/) is essential. Hugging Face \n",
       "has created a unique space where researchers not only share their work but can also engage with the community by \n",
       "upvoting, commenting, and discussing with others. This platform helps users discover the latest breakthroughs in \n",
       "AI, allowing them to catch up on great discoveries. It also spotlights Papers on Hugging Face, which are considered\n",
       "some of the most popular and influential in the AI world. Through this article, I want to highlight the collective \n",
       "interests of researchers and practitioners on Hugging Face, presenting Papers on Hugging Face that have attracted \n",
       "attention for their innovative approaches and findings.\n",
       "\n",
       "Table of contents\n",
       "-----------------\n",
       "\n",
       "* [Language Model Reasoning](#h-language-model-reasoning)\n",
       "  + [Self-Discover: Large Language Models Self-Compose Reasoning \n",
       "Structures](#h-1-self-discover-large-language-models-self-compose-reasoning-structures)\n",
       "  + [Chain-of-Thought Reasoning Without Prompting](#h-2-chain-of-thought-reasoning-without-prompting)\n",
       "  + [ReFT: Representation Finetuning for Language Models](#h-3-reft-representation-finetuning-for-language-models)\n",
       "* [Vision-Language Models](#h-vision-language-models)\n",
       "  + [What matters when building vision-language models?](#h-4-what-matters-when-building-vision-language-models)\n",
       "  + [ShareGPT4Video: Improving Video Understanding and Generation with Better \n",
       "Captions](#h-5-sharegpt4video-improving-video-understanding-and-generation-with-better-captions)\n",
       "* [Generative Models](#h-generative-models)\n",
       "  + [Depth Anything V2](#h-6-depth-anything-v2)\n",
       "  + [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale \n",
       "Prediction](#h-7-visual-autoregressive-modeling-scalable-image-generation-via-next-scale-prediction)\n",
       "* [Model Architecture](#h-model-architecture)\n",
       "  + [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context \n",
       "Length](#h-8-megalodon-efficient-llm-pretraining-and-inference-with-unlimited-cont\n",
       "..._This content has been truncated to stay below 10000 characters_...\n",
       "QApBwgH4fH+Z6YDo/Q5QQ+4D7V/Ed/UJV0/xjTACQc+MPcbtmGhFf6k7LXSFOn/RtcbRu/B0sCgNQ6LLvyFk4Y78u0KU4eX+ORhzfj2TZpWvL5vIkTh\n",
       "N5yszM02jLURdIDx1Whr/51CPeKgS3BKfcQQUeV4oHucUv8wTVg9pPmw4z6IMKA28SW9BVdmQEDbai5MQhBQr3mrFu+nghAEhEx0D7q5oFvBjisMPxP\n",
       "E8xjVRumDAnS/42UIJVLoFiahE9BcphzkORwtxFvamHmaPQAjPyoxVsB2glP91IqJgK11q0gw/M3deY8etK1mbgYGNPdqerfDiTbypcko/6Zvn0FZNm\n",
       "VllbxHD7p99xV8PIgHVVAe3xmzk1IpCvAmkyCF1yKiTaXxBYrCRtB6E+VMMG2ryfJ57K6X3wO6Dxvdvi+gGQ14CtTNfXzBNloR7c4k5XwjQMdOFyrnw\n",
       "3v04TBoz/wL84ZQFC7vqLvs6hP0H36Tckai10cVSJiZqQO8p0o5+m9Yx/lx/9h6twGaGbVxWEfuyt4L0Nx88CA3YHh4O7P1Hlk5DvEZ/TyX6/qk9Bhi\n",
       "w0A3ygQ3pWAd56M2+srkizil6xn9x4A1rp8/uuA18cfdny8ysA11iM9tXHl2H2OAbRB9apr7F76TPFbmWBCymqyeXJqbBuguRf2DHEeF5lKf73DY3Gf\n",
       "lqLzyWTpojwYauiznva8NNHe7l6wsV3i812QL4f2kx3P4jHUmGN4RAmkIU9TLhECiXYF9ARETIwgcSwD/2vlc9AvrldA8gtGSJeix4wW0FbVgbfhLtl\n",
       "D62HWvimgvKJXiPgcxFHsBIq0mqy8f/MJTTwojMwcEiLCSKMt9D5bVqTTeRX4h2kx9dEU7d9TClXElfGD5XAHlGaK57xmA32Dc+tqMEtDDarI2cni/F\n",
       "OXSuhxcENC23DkrW4Z4A0U5pbIE7hk9IQq0K1nZ3JOfBeSQwfZLH539Hg2gp9UUbeWyGj8BWHFhwzvNTsUD1E50eh8s6Mo5lfREm7dQzjzQB42PHPwf\n",
       "PHAYvXJWmkAi28rhvSKhXQkhvJtvA+2wUrT1gLByPiltBldOKn/p6IM84CZHLbf0Kfd4jlc5YQ+ryWh0baxVBX1oiIdSQOROeRjJsE7l3H9G/7TsqFv\n",
       "y/ooy/rxGrZyVr4M96fjCIohBttrtyV/h4S20elSBokwiX/rab6WcfATgUYEilm+FzjiVEwawmoyWHN5H1fkO/kJftai0hwxtZV/YdEH5jTBbDD0q61\n",
       "ROqlO2u9D2l0W9G9hOiObg2UcwKkDHsLAjPTm8VzCiJTpoH/qKiedWHcr0BGEMrZzyDNGDp17I1XJ4V+XsOhqMy3R41mOQszv07cdzorky9VLb2owH8\n",
       "kF+NXvA9s19rOpZFOHy4trU54EKMyyWpnkWaOHhjT8LztfK1B/jGuIjDAlDArbD22vMr0/9mLr70ossb3xrNAagD/1RMkhIiAJ41tETEt4N0P6q8l7H\n",
       "ZRISBgO0w0iZSUgYC6CONtwmPzEhISEhISFhhPgfihLY+meSzmwAAAAASUVORK5CYII=)\n",
       "\n",
       "Enter email address to continue\n",
       "-------------------------------\n",
       "\n",
       "Email address\n",
       "\n",
       "Get OTP\n",
       "\n",
       "![Av Logo \n",
       "White](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAKcAAAAwCAYAAAB0dWoXAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs\n",
       "4c6QAAAARnQU1BAACxjwv8YQUAAAj0SURBVHgB7VzvdeM2DGf6+r3uBFUmqDvBqRNcbgI7EySdIMoEyU1gZ4IkE8g3QXITyJ0g7gQoEYFnCAJFylYcO\n",
       "cffe0xk/oFACAJBkJQxCQkJCQkJCR8EJ+ZIAQCZ/ffZplObnk5OTu7MnrA0F+znagiaQ8HyNrX/LujnxvL2j3kjkGwv6T6FSeiGFdjEprlNtzZV0Mal\n",
       "2QNEm6MyI4LlJz8Ub5b+cii57oNfzUiBymj/5ZQ+2TQNNMF6t2Z3zMTvDBXCWo6V+WAgy4hpbfu3VqrwvBeTsAUKz6YX6IedhUj3c3hi16UZCYa0nMw\n",
       "yPnjK3Sg1M++IsVrO3KYJ+72xaUUJy86UNijQibUEG9MfBbtGX+7G1JYaFWJqaT5rjZgF2rg6LA/hs0yu/YTq/uhrH0st7rXR+KR7TD11PqESsjKnrN\n",
       "hm7e7h64O4fxcPUfWOBrZDBdT+Zc7yMujGZ9MT0LTSFbu3w21H2yWztkinVHi6Udrhi1TpXXjlRWvTspwi74UU0ccjYqbkNWgq5YXC+xX4R7a54LmEH\n",
       "v08WihCraBWYIfezjs0J0Lu4U1gK3z1oSv8VOBHIdqFXjKQDw48wzo0H/5nhUeuRJnCt0ZTVU7ofqkc5j362Ok2/WKOBCRY6QMVNvHh4U/TH1fs+hv+\n",
       "IdfAhZFQMWOUPrPp2qa/KF2zMsn3hmh+MXUozKVzVucSPC+FwCNvwwtIURyNpWd4Pj+pcWrCwFEko2vsw7Vp8v+3qV0vyctXLMeb2P+/m1o+2Ndv5iM\n",
       "A2sNDSfmNyUxPmtwaLTrKXjztO0MuwKyWieeJT8gyDz/cynmtvL2+Z22mHr7ngX4VLL9i+bNAP5ZaP/pgtKEkDhJgLrJfrQxaA1uObzF3/GNxwa7vSI\n",
       "hzlufoogKEwkobT96rsoCYWJAS4cQus+kPyv7X9ARaeUsLR4+c7jW36ZbRRzzvOwGB7eTN3Te0QLFm16Vtv6Lr/0wtl2XXZPFoAG0/R1q5su9bCk2L6\n",
       "yYYc/CjVGiELFCl8WWvpxD23WQb1XIqZaXSl1lPvluWE5ryCobtIC4ceNVFY/Q+JwkvY1lr0/TnEN/ZdazfWSjXFx31UQFyMwzuzbZPGL7BUQDdAuxX\n",
       "71AYWXTXDvlEK+cUcj3QMiznawIBf5isIvqW2D+02mtKnE7RRecYhnX5dt0pw8HS1OvsiO8mDp/YtXPMvyj1UGmc0uIwuTJ7AJoxPxxuv4hy5Cs3/YG\n",
       "TDierG0ZjZQYAuQ8/3BRTu1CrQJu1UWRK1j0P0Rm15SSzn7GstbYRAf0pnG1SWpsw3Tmj+8P3wf8ymVrxHWYQN4OOxUTwNZF5PfDArs/Y9XWgXZ8Ix1\n",
       "d2vQA9dOV87EnHSDOkDA8P8lkq4aMMspwm6OYR9Uvpg1H+rj4n98Vwdo7D2wLaPhpv4/U5PXwCeOKI0IwNOx7uPf0qWL4W53yh+1Z0Pae6maBfUuJ9x\n",
       "DbHp6jQDhTfmwEAO6xRgyesBLsr5xn4UXraxCjnpaA189TTJiuxK0Sa0QApBwgH4fH+Z6YDo/Q5QQ+4D7V/Ed/UJV0/xjTACQc+MPcbtmGhFf6k7LXS\n",
       "FOn/RtcbRu/B0sCgNQ6LLvyFk4Y78u0KU4eX+ORhzfj2TZpWvL5vIkThN5yszM02jLURdIDx1Whr/51CPeKgS3BKfcQQUeV4oHucUv8wTVg9pPmw4z6\n",
       "IMKA28SW9BVdmQEDbai5MQhBQr3mrFu+nghAEhEx0D7q5oFvBjisMPxPE8xjVRumDAnS/42UIJVLoFiahE9BcphzkORwtxFvamHmaPQAjPyoxVsB2gl\n",
       "P91IqJgK11q0gw/M3deY8etK1mbgYGNPdqerfDiTbypcko/6Zvn0FZNmVllbxHD7p99xV8PIgHVVAe3xmzk1IpCvAmkyCF1yKiTaXxBYrCRtB6E+VMM\n",
       "G2ryfJ57K6X3wO6Dxvdvi+gGQ14CtTNfXzBNloR7c4k5XwjQMdOFyrnw3v04TBoz/wL84ZQFC7vqLvs6hP0H36Tckai10cVSJiZqQO8p0o5+m9Yx/lx\n",
       "/9h6twGaGbVxWEfuyt4L0Nx88CA3YHh4O7P1Hlk5DvEZ/TyX6/qk9Bhiw0A3ygQ3pWAd56M2+srkizil6xn9x4A1rp8/uuA18cfdny8ysA11iM9tXHl\n",
       "2H2OAbRB9apr7F76TPFbmWBCymqyeXJqbBuguRf2DHEeF5lKf73DY3GflqLzyWTpojwYauiznva8NNHe7l6wsV3i812QL4f2kx3P4jHUmGN4RAmkIU9\n",
       "TLhECiXYF9ARETIwgcSwD/2vlc9AvrldA8gtGSJeix4wW0FbVgbfhLtlD62HWvimgvKJXiPgcxFHsBIq0mqy8f/MJTTwojMwcEiLCSKMt9D5bVqTTeR\n",
       "X4h2kx9dEU7d9TClXElfGD5XAHlGaK57xmA32Dc+tqMEtDDarI2cni/FOXSuhxcENC23DkrW4Z4A0U5pbIE7hk9IQq0K1nZ3JOfBeSQwfZLH539Hg2g\n",
       "p9UUbeWyGj8BWHFhwzvNTsUD1E50eh8s6Mo5lfREm7dQzjzQB42PHPwfPHAYvXJWmkAi28rhvSKhXQkhvJtvA+2wUrT1gLByPiltBldOKn/p6IM84CZ\n",
       "HLbf0Kfd4jlc5YQ+ryWh0baxVBX1oiIdSQOROeRjJsE7l3H9G/7TsqFvy/ooy/rxGrZyVr4M96fjCIohBttrtyV/h4S20elSBokwiX/rab6WcfATgUY\n",
       "Eilm+FzjiVEwawmoyWHN5H1fkO/kJftai0hwxtZV/YdEH5jTBbDD0q61ROqlO2u9D2l0W9G9hOiObg2UcwKkDHsLAjPTm8VzCiJTpoH/qKiedWHcr0B\n",
       "GEMrZzyDNGDp17I1XJ4V+XsOhqMy3R41mOQszv07cdzorky9VLb2owH8kF+NXvA9s19rOpZFOHy4trU54EKMyyWpnkWaOHhjT8LztfK1B/jGuIjDAlD\n",
       "ArbD22vMr0/9mLr70ossb3xrNAagD/1RMkhIiAJ41tETEt4N0P6q8l7HZRISBgO0w0iZSUgYC6CONtwmPzEhISEhISFhhPgfihLY+meSzmwAAAAASUV\n",
       "ORK5CYII=)\n",
       "\n",
       "Enter OTP sent to\n",
       "-----------------\n",
       "\n",
       "Edit\n",
       "\n",
       "### Enter the OTP\n",
       "\n",
       "Resend OTP\n",
       "\n",
       "Resend OTP in 45s\n",
       "\n",
       "Verify OTP\n",
       "Content from thenote.app: Top 9 Upvoted Papers on Hugging Face\n",
       "\n",
       "[TheNote.app](/en)\n",
       "\n",
       "[Sign In](/en)\n",
       "Sign Up\n",
       "\n",
       "[RSS Analytics Vidhya](/note?id=CdXz5zHNQW_caZMMxsE0e)\n",
       "\n",
       "Top 9 Upvoted Papers on Hugging Face\n",
       "====================================\n",
       "\n",
       "The field of artificial intelligence is changing rapidly. Therefore, to keep abreast of the most recent research, \n",
       "reviewing Papers on Hugging Face is essential. Hugging Face has created a unique space where researchers not only \n",
       "share their work but can also engage with the community by upvoting, commenting, and discussing with others. This \n",
       "platform helps […][www.analyticsvidhya.com\n",
       "www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2025/01/top-upvoted-papers-on-hugging-face/)[bsky.app\n",
       "Read AI and ML News on Bluesky @ai-news.at.thenote.app](https://bsky.app/profile/ai-news.at.thenote.app)\n",
       "\n",
       "[![TheNote.app (macOS, iOS and Android \n",
       "apps)](/static/avas/CdXz5zHNQWeH2WpnNh7UWg_small.jpg)](/contact?id=CdXz5zHNQW)\n",
       "[TheNote.app (macOS, iOS and Android apps)](/contact?id=CdXz5zHNQW)\n",
       "2025-01-06\n",
       "\n",
       "* [ATTACHED](#attached)\n",
       "  -\n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  -\n",
       "\n",
       "Create attached notes ...\n",
       "\n",
       "© 2015-2025, TheNote.app\n",
       "·\n",
       "[Privacy Policy](https://thenote.app/note?id=Ls7pFqImSN_eqq51y6R4n)\n",
       "·\n",
       "[Terms &amp; Conditions](https://thenote.app/note?id=Ls7pFqImSN_jQkHFE0KHN)\n",
       "·\n",
       "[Contact](mailto:admin@thenote.app)\n",
       "·\n",
       "[Android](https://play.google.com/store/apps/details?id=app.thenote&amp;referrer=utm_source%3Dtna%26utm_medium%3Dbanner\n",
       "%26utm_term%3Dindex%26utm_content%3Dindex%26utm_campaign%3Dfooter)\n",
       "[iOS](https://apps.apple.com/app/id1596565331#?platform=iphone)\n",
       "[MacOS](https://apps.apple.com/app/id1596565331#?platform=mac)\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Content from Analytics Vidhya: Top 9 Upvoted Papers on Hugging Face in 2025\n",
       "\n",
       " \n",
       "\n",
       "[![Analytics \n",
       "Vidhya](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/icon/av-logo-svg.svg)](https://www.analy\n",
       "ticsvidhya.com/blog/)\n",
       "\n",
       "* \n",
       "[DeepSeek](https://courses.analyticsvidhya.com/courses/getting-started-with-deepseek?utm_source=blog&utm_medium=nav\n",
       "bar&utm_campaign=deepseek-free-course&utm_term=29-Jan-2024)\n",
       "* [Learning Paths](https://www.analyticsvidhya.com/blog/category/learning-path/?ref=navbar)\n",
       "* [GenAI Pinnacle Plus Program](https://www.analyticsvidhya.com/pinnacleplus/?ref=blognavbar)\n",
       "  New\n",
       "* [Agentic AI Pioneer Program](https://www.analyticsvidhya.com/agenticaipioneer/?ref=blognavbar)\n",
       "\n",
       "* Login\n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  + Switch Mode\n",
       "  + [Logout](#)\n",
       "\n",
       "[Interview \n",
       "Prep](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=category)[Career](https://www.analytic\n",
       "svidhya.com/blog/category/career/?ref=category)[GenAI](https://www.analyticsvidhya.com/blog/category/generative-ai/\n",
       "?ref=category)[Prompt \n",
       "Engg](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=category)[ChatGPT](https://www.analytic\n",
       "svidhya.com/blog/category/chatgpt/?ref=category)[LLM](https://www.analyticsvidhya.com/blog/category/llms/?ref=categ\n",
       "ory)[Langchain](https://www.analyticsvidhya.com/blog/category/langchain/?ref=category)[RAG](https://www.analyticsvi\n",
       "dhya.com/blog/category/rag/?ref=category)[AI \n",
       "Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/?ref=category)[Machine \n",
       "Learning](https://www.analyticsvidhya.com/blog/category/machine-learning/?ref=category)[Deep \n",
       "Learning](https://www.analyticsvidhya.com/blog/category/deep-learning/?ref=category)[GenAI \n",
       "Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=category)[LLMOps](https://www.analyticsvidhya.co\n",
       "m/blog/category/llmops/?ref=category)[Python](https://www.analyticsvidhya.com/blog/category/python/?ref=category)[N\n",
       "LP](https://www.analyticsvidhya.com/blog/category/nlp/?ref=category)[SQL](https://www.analyticsvidhya.com/blog/cate\n",
       "gory/sql/?ref=category)[AIML Projects](https://www.analyticsvidhya.com/blog/category/project/?ref=category) \n",
       "\n",
       "![](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/av_loader.gif)\n",
       "\n",
       "1. [Home](https://www.analyticsvidhya.com/blog/)\n",
       "2. [Research Paper](https://www.analyticsvidhya.com/blog/category/research-paper/)\n",
       "3. Top 9 Upvoted Papers on Hugging Face\n",
       "\n",
       "Top 9 Upvoted Papers on Hugging Face\n",
       "====================================\n",
       "\n",
       "[![Yana \n",
       "Khare](https://av-eks-lekhak.s3.amazonaws.com/media/lekhak-profile-images/converted_image_gDtaWsc.webp)](https://ww\n",
       "w.analyticsvidhya.com/blog/author/yana_khare/)\n",
       "\n",
       "[Yana Khare](https://www.analyticsvidhya.com/blog/author/yana_khare/) \n",
       " Last Updated :\n",
       "07 Jan, 2025\n",
       "\n",
       "  8  min read\n",
       "\n",
       "The field of [artificial \n",
       "intelligence](https://www.analyticsvidhya.com/blog/2021/09/introduction-to-artificial-intelligence-for-beginners/) \n",
       "is changing rapidly. Therefore, to keep abreast of the most recent research, reviewing Papers on [Hugging \n",
       "Face](https://www.analyticsvidhya.com/blog/2023/10/hugging-face-fine-tuning-tutorial/) is essential. Hugging Face \n",
       "has created a unique space where researchers not only share their work but can also engage with the community by \n",
       "upvoting, commenting, and discussing with others. This platform helps users discover the latest breakthroughs in \n",
       "AI, allowing them to catch up on great discoveries. It also spotlights Papers on Hugging Face, which are considered\n",
       "some of the most popular and influential in the AI world. Through this article, I want to highlight the collective \n",
       "interests of researchers and practitioners on Hugging Face, presenting Papers on Hugging Face that have attracted \n",
       "attention for their innovative approaches and findings.\n",
       "\n",
       "Table of contents\n",
       "-----------------\n",
       "\n",
       "* [Language Model Reasoning](#h-language-model-reasoning)\n",
       "  + [Self-Discover: Large Language Models Self-Compose Reasoning \n",
       "Structures](#h-1-self-discover-large-language-models-self-compose-reasoning-structures)\n",
       "  + [Chain-of-Thought Reasoning Without Prompting](#h-2-chain-of-thought-reasoning-without-prompting)\n",
       "  + [ReFT: Representation Finetuning for Language Models](#h-3-reft-representation-finetuning-for-language-models)\n",
       "* [Vision-Language Models](#h-vision-language-models)\n",
       "  + [What matters when building vision-language models?](#h-4-what-matters-when-building-vision-language-models)\n",
       "  + [ShareGPT4Video: Improving Video Understanding and Generation with Better \n",
       "Captions](#h-5-sharegpt4video-improving-video-understanding-and-generation-with-better-captions)\n",
       "* [Generative Models](#h-generative-models)\n",
       "  + [Depth Anything V2](#h-6-depth-anything-v2)\n",
       "  + [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale \n",
       "Prediction](#h-7-visual-autoregressive-modeling-scalable-image-generation-via-next-scale-prediction)\n",
       "* [Model Architecture](#h-model-architecture)\n",
       "  + [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context \n",
       "Length](#h-8-megalodon-efficient-llm-pretraining-and-inference-with-unlimited-cont\n",
       "..._This content has been truncated to stay below 10000 characters_...\n",
       "QApBwgH4fH+Z6YDo/Q5QQ+4D7V/Ed/UJV0/xjTACQc+MPcbtmGhFf6k7LXSFOn/RtcbRu/B0sCgNQ6LLvyFk4Y78u0KU4eX+ORhzfj2TZpWvL5vIkTh\n",
       "N5yszM02jLURdIDx1Whr/51CPeKgS3BKfcQQUeV4oHucUv8wTVg9pPmw4z6IMKA28SW9BVdmQEDbai5MQhBQr3mrFu+nghAEhEx0D7q5oFvBjisMPxP\n",
       "E8xjVRumDAnS/42UIJVLoFiahE9BcphzkORwtxFvamHmaPQAjPyoxVsB2glP91IqJgK11q0gw/M3deY8etK1mbgYGNPdqerfDiTbypcko/6Zvn0FZNm\n",
       "VllbxHD7p99xV8PIgHVVAe3xmzk1IpCvAmkyCF1yKiTaXxBYrCRtB6E+VMMG2ryfJ57K6X3wO6Dxvdvi+gGQ14CtTNfXzBNloR7c4k5XwjQMdOFyrnw\n",
       "3v04TBoz/wL84ZQFC7vqLvs6hP0H36Tckai10cVSJiZqQO8p0o5+m9Yx/lx/9h6twGaGbVxWEfuyt4L0Nx88CA3YHh4O7P1Hlk5DvEZ/TyX6/qk9Bhi\n",
       "w0A3ygQ3pWAd56M2+srkizil6xn9x4A1rp8/uuA18cfdny8ysA11iM9tXHl2H2OAbRB9apr7F76TPFbmWBCymqyeXJqbBuguRf2DHEeF5lKf73DY3Gf\n",
       "lqLzyWTpojwYauiznva8NNHe7l6wsV3i812QL4f2kx3P4jHUmGN4RAmkIU9TLhECiXYF9ARETIwgcSwD/2vlc9AvrldA8gtGSJeix4wW0FbVgbfhLtl\n",
       "D62HWvimgvKJXiPgcxFHsBIq0mqy8f/MJTTwojMwcEiLCSKMt9D5bVqTTeRX4h2kx9dEU7d9TClXElfGD5XAHlGaK57xmA32Dc+tqMEtDDarI2cni/F\n",
       "OXSuhxcENC23DkrW4Z4A0U5pbIE7hk9IQq0K1nZ3JOfBeSQwfZLH539Hg2gp9UUbeWyGj8BWHFhwzvNTsUD1E50eh8s6Mo5lfREm7dQzjzQB42PHPwf\n",
       "PHAYvXJWmkAi28rhvSKhXQkhvJtvA+2wUrT1gLByPiltBldOKn/p6IM84CZHLbf0Kfd4jlc5YQ+ryWh0baxVBX1oiIdSQOROeRjJsE7l3H9G/7TsqFv\n",
       "y/ooy/rxGrZyVr4M96fjCIohBttrtyV/h4S20elSBokwiX/rab6WcfATgUYEilm+FzjiVEwawmoyWHN5H1fkO/kJftai0hwxtZV/YdEH5jTBbDD0q61\n",
       "ROqlO2u9D2l0W9G9hOiObg2UcwKkDHsLAjPTm8VzCiJTpoH/qKiedWHcr0BGEMrZzyDNGDp17I1XJ4V+XsOhqMy3R41mOQszv07cdzorky9VLb2owH8\n",
       "kF+NXvA9s19rOpZFOHy4trU54EKMyyWpnkWaOHhjT8LztfK1B/jGuIjDAlDArbD22vMr0/9mLr70ossb3xrNAagD/1RMkhIiAJ41tETEt4N0P6q8l7H\n",
       "ZRISBgO0w0iZSUgYC6CONtwmPzEhISEhISFhhPgfihLY+meSzmwAAAAASUVORK5CYII=)\n",
       "\n",
       "Enter email address to continue\n",
       "-------------------------------\n",
       "\n",
       "Email address\n",
       "\n",
       "Get OTP\n",
       "\n",
       "![Av Logo \n",
       "White](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAKcAAAAwCAYAAAB0dWoXAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs\n",
       "4c6QAAAARnQU1BAACxjwv8YQUAAAj0SURBVHgB7VzvdeM2DGf6+r3uBFUmqDvBqRNcbgI7EySdIMoEyU1gZ4IkE8g3QXITyJ0g7gQoEYFnCAJFylYcO\n",
       "cffe0xk/oFACAJBkJQxCQkJCQkJCR8EJ+ZIAQCZ/ffZplObnk5OTu7MnrA0F+znagiaQ8HyNrX/LujnxvL2j3kjkGwv6T6FSeiGFdjEprlNtzZV0Mal\n",
       "2QNEm6MyI4LlJz8Ub5b+cii57oNfzUiBymj/5ZQ+2TQNNMF6t2Z3zMTvDBXCWo6V+WAgy4hpbfu3VqrwvBeTsAUKz6YX6IedhUj3c3hi16UZCYa0nMw\n",
       "yPnjK3Sg1M++IsVrO3KYJ+72xaUUJy86UNijQibUEG9MfBbtGX+7G1JYaFWJqaT5rjZgF2rg6LA/hs0yu/YTq/uhrH0st7rXR+KR7TD11PqESsjKnrN\n",
       "hm7e7h64O4fxcPUfWOBrZDBdT+Zc7yMujGZ9MT0LTSFbu3w21H2yWztkinVHi6Udrhi1TpXXjlRWvTspwi74UU0ccjYqbkNWgq5YXC+xX4R7a54LmEH\n",
       "v08WihCraBWYIfezjs0J0Lu4U1gK3z1oSv8VOBHIdqFXjKQDw48wzo0H/5nhUeuRJnCt0ZTVU7ofqkc5j362Ok2/WKOBCRY6QMVNvHh4U/TH1fs+hv+\n",
       "IdfAhZFQMWOUPrPp2qa/KF2zMsn3hmh+MXUozKVzVucSPC+FwCNvwwtIURyNpWd4Pj+pcWrCwFEko2vsw7Vp8v+3qV0vyctXLMeb2P+/m1o+2Ndv5iM\n",
       "A2sNDSfmNyUxPmtwaLTrKXjztO0MuwKyWieeJT8gyDz/cynmtvL2+Z22mHr7ngX4VLL9i+bNAP5ZaP/pgtKEkDhJgLrJfrQxaA1uObzF3/GNxwa7vSI\n",
       "hzlufoogKEwkobT96rsoCYWJAS4cQus+kPyv7X9ARaeUsLR4+c7jW36ZbRRzzvOwGB7eTN3Te0QLFm16Vtv6Lr/0wtl2XXZPFoAG0/R1q5su9bCk2L6\n",
       "yYYc/CjVGiELFCl8WWvpxD23WQb1XIqZaXSl1lPvluWE5ryCobtIC4ceNVFY/Q+JwkvY1lr0/TnEN/ZdazfWSjXFx31UQFyMwzuzbZPGL7BUQDdAuxX\n",
       "71AYWXTXDvlEK+cUcj3QMiznawIBf5isIvqW2D+02mtKnE7RRecYhnX5dt0pw8HS1OvsiO8mDp/YtXPMvyj1UGmc0uIwuTJ7AJoxPxxuv4hy5Cs3/YG\n",
       "TDierG0ZjZQYAuQ8/3BRTu1CrQJu1UWRK1j0P0Rm15SSzn7GstbYRAf0pnG1SWpsw3Tmj+8P3wf8ymVrxHWYQN4OOxUTwNZF5PfDArs/Y9XWgXZ8Ix1\n",
       "d2vQA9dOV87EnHSDOkDA8P8lkq4aMMspwm6OYR9Uvpg1H+rj4n98Vwdo7D2wLaPhpv4/U5PXwCeOKI0IwNOx7uPf0qWL4W53yh+1Z0Pae6maBfUuJ9x\n",
       "DbHp6jQDhTfmwEAO6xRgyesBLsr5xn4UXraxCjnpaA189TTJiuxK0Sa0QApBwgH4fH+Z6YDo/Q5QQ+4D7V/Ed/UJV0/xjTACQc+MPcbtmGhFf6k7LXS\n",
       "FOn/RtcbRu/B0sCgNQ6LLvyFk4Y78u0KU4eX+ORhzfj2TZpWvL5vIkThN5yszM02jLURdIDx1Whr/51CPeKgS3BKfcQQUeV4oHucUv8wTVg9pPmw4z6\n",
       "IMKA28SW9BVdmQEDbai5MQhBQr3mrFu+nghAEhEx0D7q5oFvBjisMPxPE8xjVRumDAnS/42UIJVLoFiahE9BcphzkORwtxFvamHmaPQAjPyoxVsB2gl\n",
       "P91IqJgK11q0gw/M3deY8etK1mbgYGNPdqerfDiTbypcko/6Zvn0FZNmVllbxHD7p99xV8PIgHVVAe3xmzk1IpCvAmkyCF1yKiTaXxBYrCRtB6E+VMM\n",
       "G2ryfJ57K6X3wO6Dxvdvi+gGQ14CtTNfXzBNloR7c4k5XwjQMdOFyrnw3v04TBoz/wL84ZQFC7vqLvs6hP0H36Tckai10cVSJiZqQO8p0o5+m9Yx/lx\n",
       "/9h6twGaGbVxWEfuyt4L0Nx88CA3YHh4O7P1Hlk5DvEZ/TyX6/qk9Bhiw0A3ygQ3pWAd56M2+srkizil6xn9x4A1rp8/uuA18cfdny8ysA11iM9tXHl\n",
       "2H2OAbRB9apr7F76TPFbmWBCymqyeXJqbBuguRf2DHEeF5lKf73DY3GflqLzyWTpojwYauiznva8NNHe7l6wsV3i812QL4f2kx3P4jHUmGN4RAmkIU9\n",
       "TLhECiXYF9ARETIwgcSwD/2vlc9AvrldA8gtGSJeix4wW0FbVgbfhLtlD62HWvimgvKJXiPgcxFHsBIq0mqy8f/MJTTwojMwcEiLCSKMt9D5bVqTTeR\n",
       "X4h2kx9dEU7d9TClXElfGD5XAHlGaK57xmA32Dc+tqMEtDDarI2cni/FOXSuhxcENC23DkrW4Z4A0U5pbIE7hk9IQq0K1nZ3JOfBeSQwfZLH539Hg2g\n",
       "p9UUbeWyGj8BWHFhwzvNTsUD1E50eh8s6Mo5lfREm7dQzjzQB42PHPwfPHAYvXJWmkAi28rhvSKhXQkhvJtvA+2wUrT1gLByPiltBldOKn/p6IM84CZ\n",
       "HLbf0Kfd4jlc5YQ+ryWh0baxVBX1oiIdSQOROeRjJsE7l3H9G/7TsqFvy/ooy/rxGrZyVr4M96fjCIohBttrtyV/h4S20elSBokwiX/rab6WcfATgUY\n",
       "Eilm+FzjiVEwawmoyWHN5H1fkO/kJftai0hwxtZV/YdEH5jTBbDD0q61ROqlO2u9D2l0W9G9hOiObg2UcwKkDHsLAjPTm8VzCiJTpoH/qKiedWHcr0B\n",
       "GEMrZzyDNGDp17I1XJ4V+XsOhqMy3R41mOQszv07cdzorky9VLb2owH8kF+NXvA9s19rOpZFOHy4trU54EKMyyWpnkWaOHhjT8LztfK1B/jGuIjDAlD\n",
       "ArbD22vMr0/9mLr70ossb3xrNAagD/1RMkhIiAJ41tETEt4N0P6q8l7HZRISBgO0w0iZSUgYC6CONtwmPzEhISEhISFhhPgfihLY+meSzmwAAAAASUV\n",
       "ORK5CYII=)\n",
       "\n",
       "Enter OTP sent to\n",
       "-----------------\n",
       "\n",
       "Edit\n",
       "\n",
       "### Enter the OTP\n",
       "\n",
       "Resend OTP\n",
       "\n",
       "Resend OTP in 45s\n",
       "\n",
       "Verify OTP\n",
       "Content from thenote.app: Top 9 Upvoted Papers on Hugging Face\n",
       "\n",
       "[TheNote.app](/en)\n",
       "\n",
       "[Sign In](/en)\n",
       "Sign Up\n",
       "\n",
       "[RSS Analytics Vidhya](/note?id=CdXz5zHNQW_caZMMxsE0e)\n",
       "\n",
       "Top 9 Upvoted Papers on Hugging Face\n",
       "====================================\n",
       "\n",
       "The field of artificial intelligence is changing rapidly. Therefore, to keep abreast of the most recent research, \n",
       "reviewing Papers on Hugging Face is essential. Hugging Face has created a unique space where researchers not only \n",
       "share their work but can also engage with the community by upvoting, commenting, and discussing with others. This \n",
       "platform helps […][www.analyticsvidhya.com\n",
       "www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2025/01/top-upvoted-papers-on-hugging-face/)[bsky.app\n",
       "Read AI and ML News on Bluesky @ai-news.at.thenote.app](https://bsky.app/profile/ai-news.at.thenote.app)\n",
       "\n",
       "[![TheNote.app (macOS, iOS and Android \n",
       "apps)](/static/avas/CdXz5zHNQWeH2WpnNh7UWg_small.jpg)](/contact?id=CdXz5zHNQW)\n",
       "[TheNote.app (macOS, iOS and Android apps)](/contact?id=CdXz5zHNQW)\n",
       "2025-01-06\n",
       "\n",
       "* [ATTACHED](#attached)\n",
       "  -\n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  \n",
       "  -\n",
       "\n",
       "Create attached notes ...\n",
       "\n",
       "© 2015-2025, TheNote.app\n",
       "·\n",
       "[Privacy Policy](https://thenote.app/note?id=Ls7pFqImSN_eqq51y6R4n)\n",
       "·\n",
       "[Terms & Conditions](https://thenote.app/note?id=Ls7pFqImSN_jQkHFE0KHN)\n",
       "·\n",
       "[Contact](mailto:admin@thenote.app)\n",
       "·\n",
       "[Android](https://play.google.com/store/apps/details?id=app.thenote&referrer=utm_source%3Dtna%26utm_medium%3Dbanner\n",
       "%26utm_term%3Dindex%26utm_content%3Dindex%26utm_campaign%3Dfooter)\n",
       "[iOS](https://apps.apple.com/app/id1596565331#?platform=iphone)\n",
       "[MacOS](https://apps.apple.com/app/id1596565331#?platform=mac)\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 16.64 seconds| Input tokens: 9,546 | Output tokens: 629]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 16.64 seconds| Input tokens: 9,546 | Output tokens: 629]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m5\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Self-Discover: Large Language Models Self-Compose Reasoning Structures\"</span><span style=\"background-color: #272822\">                     </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_paper_id_by_title(title</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title)</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Paper ID:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, paper_id)</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">download_paper_by_id(paper_id</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id)</span><span style=\"background-color: #272822\">                                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Paper downloaded. Reading the paper...\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_content </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> read_pdf_file(file_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"paper.pdf\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Paper content:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, paper_content)</span><span style=\"background-color: #272822\">                                                                         </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mSelf-Discover: Large Language Models Self-Compose Reasoning Structures\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                     \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_paper_id_by_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper ID:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mdownload_paper_by_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper downloaded. Reading the paper...\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mread_pdf_file\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile_path\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mpaper.pdf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper content:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Paper ID: 2402.03620\n",
       "Paper downloaded. Reading the paper...\n",
       "Paper content: SELF -DISCOVER : Large Language Models Self-Compose Reasoning Structures\n",
       "Pei Zhou 1 Jay Pujara 1 Xiang Ren 1 Xinyun Chen 2 Heng-Tze Cheng 2\n",
       "Quoc V . Le2 Ed H. Chi 2 Denny Zhou 2 Swaroop Mishra 2 Huaixiu Steven Zheng 2\n",
       "Abstract\n",
       "We introduce SELF -DISCOVER , a general frame-\n",
       "work for LLMs to self-discover the task-intrinsic\n",
       "reasoning structures to tackle complex reasoning\n",
       "problems that are challenging for typical prompt-\n",
       "ing methods. Core to the framework is a self-\n",
       "discovery process where LLMs select multiple\n",
       "atomic reasoning modules such as critical think-\n",
       "ing and step-by-step thinking, and compose them\n",
       "into an explicit reasoning structure for LLMs to\n",
       "follow during decoding. SELF -DISCOVER sub-\n",
       "stantially improves GPT-4 and PaLM 2’s per-\n",
       "formance on challenging reasoning benchmarks\n",
       "such as BigBench-Hard, grounded agent reason-\n",
       "ing, and MATH, by as much as 32% compared\n",
       "to Chain of Thought (CoT). Furthermore, SELF -\n",
       "DISCOVER outperforms inference-intensive meth-\n",
       "ods such as CoT-Self-Consistency by more than\n",
       "20%, while requiring 10-40x fewer inference com-\n",
       "pute. Finally, we show that the self-discovered\n",
       "reasoning structures are universally applicable\n",
       "across model families: from PaLM 2-L to GPT-4,\n",
       "and from GPT-4 to Llama2, and share commonal-\n",
       "ities with human reasoning patterns.\n",
       "1. Introduction\n",
       "Large Language Models (LLM) (Brown et al., 2020; Chowd-\n",
       "hery et al., 2022; OpenAI, 2023b; Anil et al., 2023) pow-\n",
       "ered by transformers (Vaswani et al., 2017) have produced\n",
       "impressive breakthroughs in generating coherent texts (Ope-\n",
       "nAI, 2022), and following instructions (Zhong et al., 2021;\n",
       "Mishra et al., 2022c; Wei et al., 2021; Chung et al., 2022;\n",
       "Ouyang et al., 2022). In pursuit of the goal to enhance\n",
       "LLMs’ capability to reason and solve complex problems,\n",
       "various prompting methods have been proposed, drawing\n",
       "inspirations from cognitive theories of how humans rea-\n",
       "1University of Southern California 2Google DeepMind. Cor-\n",
       "respondence to: Pei Zhou &lt;peiz@usc.edu&gt;, Swaroop Mishra\n",
       "&lt;swaroopmishra@google.com&gt;, Huaixiu Steven Zheng &lt; steven-\n",
       "zheng@google.com&gt;.\n",
       "Preprint.\n",
       "son. For example, few-shot and zero-shot chain-of-thought\n",
       "(CoT) (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022;\n",
       "Yasunaga et al., 2023) resembles how humans solve prob-\n",
       "lems step-by-step, decomposition-based prompting (Zhou\n",
       "et al., 2022a; Drozdov et al., 2022; Patel et al., 2022; Hao\n",
       "et al., 2023; Khot et al., 2022) is inspired by how humans\n",
       "breakdown a complex problem into a series of smaller\n",
       "subproblems, and then solve those subproblems one by\n",
       "one (Polya, 2004), and step-back prompting (Zheng et al.,\n",
       "2023) is motivated by how humans reflect on task nature\n",
       "to derive general principles. However, a fundamental limi-\n",
       "tation is that each technique itself serves as an atomic rea-\n",
       "soning module making an implicit prior assumption of the\n",
       "process on how to tackle a given task. Instead, we argue\n",
       "that each task has a unique intrinsic structure underlying\n",
       "the reasoning process involved in solving it efficiently. For\n",
       "instance, least-to-most prompting (Zhou et al., 2022a; Droz-\n",
       "dov et al., 2022) has shown to be much more effective than\n",
       "CoT (Wei et al., 2022) at solving tasks such as symbolic\n",
       "manipulation and compositional generalization, due to the\n",
       "decomposition structure of the tasks.\n",
       "This paper aims at self-discovering the underlying reasoning\n",
       "structure unique to each task, while being highly efficient in\n",
       "terms of computation. Our approach, SELF -DISCOVER , is\n",
       "inspired by how humans internally devise a reasoning pro-\n",
       "gram for problem-solving (Newell et al., 1958; Rasmussen,\n",
       "1983), as illustrated in Figure 2 . From a set of atomic\n",
       "reasoning modules described in natural language such as\n",
       "“breakdown into sub tasks” and “critical thinking”, an LLM,\n",
       "and task examples without labels, SELF -DISCOVER com-\n",
       "poses a coherent reasoning structure intrinsic to the task\n",
       "(Stage 1) and then solves instances of the task using the\n",
       "discovered structure (Stage 2). Stage 1 operates at the task-\n",
       "level and uses three actions to guide the LLM to generate\n",
       "a reasoning structure for the task. At Stage 2, during the\n",
       "final decoding, the LLM simply follows the self-discovered\n",
       "structure to arrive at the final answer.\n",
       "Solving problems using SELF -DISCOVER brings several\n",
       "benefits compared to other methods for LLM reasoning.\n",
       "First, the discovered reasoning structure is grounded in\n",
       "atomic reasoning modules benefiting from the strengths of\n",
       "multiple reasoning modules in contrast to applying a priori\n",
       "module such as CoT. Second, SELF -DISCOVER is efficient\n",
       "1\n",
       "arXiv:2402.03620v1  [cs.AI]  6 Feb 2024SELF -DISCOVER : Large Language Models Self-Compose Reasoning Structures\n",
       "Task\n",
       "Chain-of-Thought \n",
       "(CoT)\n",
       "Direct Answer Answer\n",
       "Self-Discover \n",
       "Reasoning \n",
       "Structures (Ours)\n",
       "AnswerTask\n",
       "Task\n",
       " Task-Speciﬁc \n",
       "Reasoning Structure\n",
       "Rationale\n",
       "AnswerStructured \n",
       "Reasoning\n",
       "Avg. BBH: +11%\n",
       "T4D: + 39%\n",
       "MATH: +5.5%\n",
       "Avg. BBH: +7%\n",
       "T4D: + 29%\n",
       "MATH: +8.5%\n",
       "Figure 1.SELF -DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning \n",
       "structure to solve\n",
       "challenging tasks. Through testing on challenging reasoning benchmarks incuding Big Bench-Hard (BBH), agent \n",
       "reasoning (T4D), and\n",
       "MATH, we find that SELF -DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting\n",
       "using PaLM\n",
       "2-L. Full BBH results are in Appendix C Table 3.\n",
       "in computation as it only requires 3 more inference steps on\n",
       "the task-level, while being more performant than inference-\n",
       "heavy ensemble approaches such as self-consistency (Wang\n",
       "et al., 2022). Lastly, the discovered reasoning structure\n",
       "is intrinsic to the task, and conveys LLMs’ insights about\n",
       "the task in a more interpretable way than the optimized\n",
       "prompts (Zhou et al., 2022b; Yang et al., 2023).\n",
       "We test SELF -DISCOVER on 25 challenging reasoning\n",
       "tasks including Big Bench-Hard (BBH) (Suzgun et al.,\n",
       "2022), Thinking for Doing (T4D) (Zhou et al., 2023) and\n",
       "MATH (Hendrycks et al., 2021). SELF -DISCOVER outper-\n",
       "forms CoT on 21/25 task with performance gains up to 42%\n",
       "(Figure 1), highlighting the advantage of the self-discovered\n",
       "reasoning structure composed from the atomic reasoning\n",
       "modules against a single a priori CoT module. Furthermore,\n",
       "we demonstrate that SELF -DISCOVER achieves superior\n",
       "performance against inference-heavy methods such as CoT\n",
       "+ Self-Consistency and majority voting of every module\n",
       "while requiring 10-40x fewer inference compute (Figure 5).\n",
       "Finally, we compare SELF -DISCOVER with prompts op-\n",
       "timized (OPRO) using a training set (Yang et al., 2023)\n",
       "(Figure 9). We find that SELF -DISCOVER still performs on\n",
       "par or better than OPRO while the self-discovered reasoning\n",
       "structure are much more interpretable.\n",
       "We conduct a set of analysis to understand the effectiveness\n",
       "of SELF -DISCOVER . By breaking down BBH tasks into 4\n",
       "different categories, we find that SELF -DISCOVER performs\n",
       "best on tasks requiring world knowledge and has a mod-\n",
       "erate performance boost on algorithmic tasks compared to\n",
       "CoT (Figure 4). This is further confirmed by the error anal-\n",
       "ysis on MATH, where 74.7% model failures comes from\n",
       "computation errors (e.g. math). We also take a closer look\n",
       "at the self-discovered reasoning structures, and show the\n",
       "universality of them by transferability study from PaLM\n",
       "2-L to GPT-4, and from GPT-4 to Llama-2-70B. We hope\n",
       "to encourage more future work on structured reasoning for\n",
       "solving challenging problems using LLMs.\n",
       "2. Self-Discovering Reasoning Structures for\n",
       "Problem-Solving\n",
       "We take inspiration from how humans use prior knowledge\n",
       "and skills to devise a reasoning program to solve prob-\n",
       "lems (Newell et al., 1958; Rasmussen, 1983). When we\n",
       "face a new problem, we often first search internally what\n",
       "knowledge and skills from our prior experience might be\n",
       "helpful to solve it. Then we will attempt to apply relevant\n",
       "knowledge and skills to this task. And finally we will con-\n",
       "nect multiple individual skills and knowledge to solve the\n",
       "problem. We design SELF -DISCOVER to enact these steps\n",
       "into two stages as illustrated in Figure 2.\n",
       "Given a task and a set of reasoning module descriptions\n",
       "representing high-level problem-solving heuristics such as\n",
       "“Use critical thinking” and “Let’s think step by step”, Stage 1\n",
       "of SELF -DISCOVER aims to uncover the intrinsic reasoning\n",
       "structure for solving this task via meta-reasoning. Specifi-\n",
       "cally, we uses three meta-prompts to guide LLMs to select,\n",
       "adapt, and implement an actionable reasoning structure with\n",
       "no labels or training required. We format the structure in\n",
       "key-value pairs similar to JSON due to interpretability and\n",
       "findings on following JSON boosts reasoning and generation\n",
       "quality (Zhou et al., 2023; OpenAI, 2023a). The structure of\n",
       "2SELF -DISCOVER : Large Language Models Self-Compose Reasoning Structures\n",
       "Language \n",
       "Model\n",
       "Atomic Reasoning Modules\n",
       "Reasoning Structure\n",
       "{\n",
       "    \"Type and color of each item\": \"\"\n",
       "    \"Number of items of each color\": \"\"\n",
       "    \"Number of items of each type\": \"\"\n",
       "    \"Number of items of each color and type\": \n",
       "    \"Final answer\":\n",
       "}\n",
       "Task: Reasoning \n",
       "colored objects\n",
       "Self-Discover\n",
       "Language \n",
       "Model\n",
       " Answer\n",
       "Stage 1: Discover Reasoning Structure on Task-Level\n",
       "Stage 2: Solve Problems Using Discovered Structure on Instance-Level\n",
       "Reasoning Structure\n",
       "Task Instance\n",
       "Key-Value pairs\n",
       "Fill in the Values based on \n",
       "Keys during decoding\n",
       "Figure 2.Illustration of using SELF -DISCOVER for problem-solving. Given a generative LM, task, and seed reasoning \n",
       "module\n",
       "descriptions, we guide LMs to generate a reasoning structure in key-value format to solve the task. Finally, models\n",
       "can follow the\n",
       "self-discovered structures to solve the every instance from the task by filling in the values in JSON step-by-step.\n",
       "the meta-prompts and full prompts are shown in Appendix.\n",
       "Stage 1 operates on task-level, meaning we only need to run\n",
       "SELF -DISCOVER once for each task. Then, in Stage 2, we\n",
       "can simply use the discovered reasoning structure to solve\n",
       "every instance of the given task by instructing models to\n",
       "follow the provided structure by filling each key and arrive\n",
       "at a final answer.\n",
       "2.1. Stage 1: Self-Discover Task-Specific Structures\n",
       "The first stage consists of three actions: 1) SELECT, where\n",
       "relevant reasoning modules for task-solving are chosen from\n",
       "the set of reasoning module descriptions; 2) ADAPT, where\n",
       "descriptions of selected reasoning modules are rephrased to\n",
       "be more specific to the task at hand; and 3) IMPLEMENT,\n",
       "where the adapted reasoning descriptions are implemented\n",
       "into a structured actionable plan so that the task can be\n",
       "solved by following the structure.\n",
       "SELECT First, not every reasoning module is helpful for\n",
       "every task, so the first stage of SELF -DISCOVER guides\n",
       "model to select modules that are useful based on task exam-\n",
       "ples. For example, “reflective thinking” might help search\n",
       "for first-principle theories on science problems, while “cre-\n",
       "ative thinking” helps on generating a novel continuation to\n",
       "a story. Given raw set of reasoning module descriptions\n",
       "D such as “critical thinking”, and “break the problem into\n",
       "sub-problems” (full set in Appendix A), and a few task ex-\n",
       "amples without labels ti ∈ T, SELF -DISCOVER first selects\n",
       "a subset of reasoning modulesDS that are useful for solving\n",
       "the tasks by using a model M and a meta-prompt pS:\n",
       "DS = M(pS ∥ D ∥ ti). (1)\n",
       "ADAPT Since each reasoning module provides a general\n",
       "description of how to solve problems, the next step of SELF -\n",
       "DISCOVER aims at tailoring each selected module to the\n",
       "task at hand. For example, from “break the problem into sub-\n",
       "problems” to “calculate each arithmetic operation in order”\n",
       "for arithmetic problems. Given selected reasoning module\n",
       "subset DS from the previous step, ADAPT rephrases each\n",
       "of the selected module to be more specific to the task. Sim-\n",
       "ilarly to SELECT, this stage uses a meta-prompt pA and\n",
       "a generative model M to generate the adapted reasoning\n",
       "module descriptions DA:\n",
       "DA = M(pA ∥ DS ∥ ti). (2)\n",
       "IMPLEMENT Finally, given the adapted reasoning mod-\n",
       "ule descriptions DA, SELF -DISCOVER operationalizes the\n",
       "reasoning modules into an implemented reasoning struc-\n",
       "ture DI with specified instruction on what to generate for\n",
       "each step. In addition to a meta prompt pI, IMPLEMENT\n",
       "also provides a demonstration of a human-written reason-\n",
       "ing structure Shuman on another task to better convert the\n",
       "natural language descriptions into a reasoning structure:\n",
       "DI = M(pA ∥ Shuman ∥ DA ∥ ti). (3)\n",
       "2.2. Stage 2: Tackle Tasks Using Discovered Structures\n",
       "After the three stages, we have an implemented reasoning\n",
       "structure DI uniquely adapted for the task we need to solve\n",
       "T. Then we can simply append the reasoning structure to\n",
       "all instances of the task and prompt models to follow the\n",
       "reasoning structure to generate an answer A:\n",
       "A = M(DS ∥ t), ∀t ∈ T. (4)\n",
       "More details of prompts are included in Appendix A.\n",
       "3. Experiment Setup\n",
       "3.1. Tasks\n",
       "We focus on diverse reasoning benchmarks that are still\n",
       "challenging for LLMs: BIG-Bench Hard (BBH) (Suzgun\n",
       "3\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Paper ID: 2402.03620\n",
       "Paper downloaded. Reading the paper...\n",
       "Paper content: SELF -DISCOVER : Large Language Models Self-Compose Reasoning Structures\n",
       "Pei Zhou 1 Jay Pujara 1 Xiang Ren 1 Xinyun Chen 2 Heng-Tze Cheng 2\n",
       "Quoc V . Le2 Ed H. Chi 2 Denny Zhou 2 Swaroop Mishra 2 Huaixiu Steven Zheng 2\n",
       "Abstract\n",
       "We introduce SELF -DISCOVER , a general frame-\n",
       "work for LLMs to self-discover the task-intrinsic\n",
       "reasoning structures to tackle complex reasoning\n",
       "problems that are challenging for typical prompt-\n",
       "ing methods. Core to the framework is a self-\n",
       "discovery process where LLMs select multiple\n",
       "atomic reasoning modules such as critical think-\n",
       "ing and step-by-step thinking, and compose them\n",
       "into an explicit reasoning structure for LLMs to\n",
       "follow during decoding. SELF -DISCOVER sub-\n",
       "stantially improves GPT-4 and PaLM 2’s per-\n",
       "formance on challenging reasoning benchmarks\n",
       "such as BigBench-Hard, grounded agent reason-\n",
       "ing, and MATH, by as much as 32% compared\n",
       "to Chain of Thought (CoT). Furthermore, SELF -\n",
       "DISCOVER outperforms inference-intensive meth-\n",
       "ods such as CoT-Self-Consistency by more than\n",
       "20%, while requiring 10-40x fewer inference com-\n",
       "pute. Finally, we show that the self-discovered\n",
       "reasoning structures are universally applicable\n",
       "across model families: from PaLM 2-L to GPT-4,\n",
       "and from GPT-4 to Llama2, and share commonal-\n",
       "ities with human reasoning patterns.\n",
       "1. Introduction\n",
       "Large Language Models (LLM) (Brown et al., 2020; Chowd-\n",
       "hery et al., 2022; OpenAI, 2023b; Anil et al., 2023) pow-\n",
       "ered by transformers (Vaswani et al., 2017) have produced\n",
       "impressive breakthroughs in generating coherent texts (Ope-\n",
       "nAI, 2022), and following instructions (Zhong et al., 2021;\n",
       "Mishra et al., 2022c; Wei et al., 2021; Chung et al., 2022;\n",
       "Ouyang et al., 2022). In pursuit of the goal to enhance\n",
       "LLMs’ capability to reason and solve complex problems,\n",
       "various prompting methods have been proposed, drawing\n",
       "inspirations from cognitive theories of how humans rea-\n",
       "1University of Southern California 2Google DeepMind. Cor-\n",
       "respondence to: Pei Zhou <peiz@usc.edu>, Swaroop Mishra\n",
       "<swaroopmishra@google.com>, Huaixiu Steven Zheng < steven-\n",
       "zheng@google.com>.\n",
       "Preprint.\n",
       "son. For example, few-shot and zero-shot chain-of-thought\n",
       "(CoT) (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022;\n",
       "Yasunaga et al., 2023) resembles how humans solve prob-\n",
       "lems step-by-step, decomposition-based prompting (Zhou\n",
       "et al., 2022a; Drozdov et al., 2022; Patel et al., 2022; Hao\n",
       "et al., 2023; Khot et al., 2022) is inspired by how humans\n",
       "breakdown a complex problem into a series of smaller\n",
       "subproblems, and then solve those subproblems one by\n",
       "one (Polya, 2004), and step-back prompting (Zheng et al.,\n",
       "2023) is motivated by how humans reflect on task nature\n",
       "to derive general principles. However, a fundamental limi-\n",
       "tation is that each technique itself serves as an atomic rea-\n",
       "soning module making an implicit prior assumption of the\n",
       "process on how to tackle a given task. Instead, we argue\n",
       "that each task has a unique intrinsic structure underlying\n",
       "the reasoning process involved in solving it efficiently. For\n",
       "instance, least-to-most prompting (Zhou et al., 2022a; Droz-\n",
       "dov et al., 2022) has shown to be much more effective than\n",
       "CoT (Wei et al., 2022) at solving tasks such as symbolic\n",
       "manipulation and compositional generalization, due to the\n",
       "decomposition structure of the tasks.\n",
       "This paper aims at self-discovering the underlying reasoning\n",
       "structure unique to each task, while being highly efficient in\n",
       "terms of computation. Our approach, SELF -DISCOVER , is\n",
       "inspired by how humans internally devise a reasoning pro-\n",
       "gram for problem-solving (Newell et al., 1958; Rasmussen,\n",
       "1983), as illustrated in Figure 2 . From a set of atomic\n",
       "reasoning modules described in natural language such as\n",
       "“breakdown into sub tasks” and “critical thinking”, an LLM,\n",
       "and task examples without labels, SELF -DISCOVER com-\n",
       "poses a coherent reasoning structure intrinsic to the task\n",
       "(Stage 1) and then solves instances of the task using the\n",
       "discovered structure (Stage 2). Stage 1 operates at the task-\n",
       "level and uses three actions to guide the LLM to generate\n",
       "a reasoning structure for the task. At Stage 2, during the\n",
       "final decoding, the LLM simply follows the self-discovered\n",
       "structure to arrive at the final answer.\n",
       "Solving problems using SELF -DISCOVER brings several\n",
       "benefits compared to other methods for LLM reasoning.\n",
       "First, the discovered reasoning structure is grounded in\n",
       "atomic reasoning modules benefiting from the strengths of\n",
       "multiple reasoning modules in contrast to applying a priori\n",
       "module such as CoT. Second, SELF -DISCOVER is efficient\n",
       "1\n",
       "arXiv:2402.03620v1  [cs.AI]  6 Feb 2024SELF -DISCOVER : Large Language Models Self-Compose Reasoning Structures\n",
       "Task\n",
       "Chain-of-Thought \n",
       "(CoT)\n",
       "Direct Answer Answer\n",
       "Self-Discover \n",
       "Reasoning \n",
       "Structures (Ours)\n",
       "AnswerTask\n",
       "Task\n",
       " Task-Speciﬁc \n",
       "Reasoning Structure\n",
       "Rationale\n",
       "AnswerStructured \n",
       "Reasoning\n",
       "Avg. BBH: +11%\n",
       "T4D: + 39%\n",
       "MATH: +5.5%\n",
       "Avg. BBH: +7%\n",
       "T4D: + 29%\n",
       "MATH: +8.5%\n",
       "Figure 1.SELF -DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning \n",
       "structure to solve\n",
       "challenging tasks. Through testing on challenging reasoning benchmarks incuding Big Bench-Hard (BBH), agent \n",
       "reasoning (T4D), and\n",
       "MATH, we find that SELF -DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting\n",
       "using PaLM\n",
       "2-L. Full BBH results are in Appendix C Table 3.\n",
       "in computation as it only requires 3 more inference steps on\n",
       "the task-level, while being more performant than inference-\n",
       "heavy ensemble approaches such as self-consistency (Wang\n",
       "et al., 2022). Lastly, the discovered reasoning structure\n",
       "is intrinsic to the task, and conveys LLMs’ insights about\n",
       "the task in a more interpretable way than the optimized\n",
       "prompts (Zhou et al., 2022b; Yang et al., 2023).\n",
       "We test SELF -DISCOVER on 25 challenging reasoning\n",
       "tasks including Big Bench-Hard (BBH) (Suzgun et al.,\n",
       "2022), Thinking for Doing (T4D) (Zhou et al., 2023) and\n",
       "MATH (Hendrycks et al., 2021). SELF -DISCOVER outper-\n",
       "forms CoT on 21/25 task with performance gains up to 42%\n",
       "(Figure 1), highlighting the advantage of the self-discovered\n",
       "reasoning structure composed from the atomic reasoning\n",
       "modules against a single a priori CoT module. Furthermore,\n",
       "we demonstrate that SELF -DISCOVER achieves superior\n",
       "performance against inference-heavy methods such as CoT\n",
       "+ Self-Consistency and majority voting of every module\n",
       "while requiring 10-40x fewer inference compute (Figure 5).\n",
       "Finally, we compare SELF -DISCOVER with prompts op-\n",
       "timized (OPRO) using a training set (Yang et al., 2023)\n",
       "(Figure 9). We find that SELF -DISCOVER still performs on\n",
       "par or better than OPRO while the self-discovered reasoning\n",
       "structure are much more interpretable.\n",
       "We conduct a set of analysis to understand the effectiveness\n",
       "of SELF -DISCOVER . By breaking down BBH tasks into 4\n",
       "different categories, we find that SELF -DISCOVER performs\n",
       "best on tasks requiring world knowledge and has a mod-\n",
       "erate performance boost on algorithmic tasks compared to\n",
       "CoT (Figure 4). This is further confirmed by the error anal-\n",
       "ysis on MATH, where 74.7% model failures comes from\n",
       "computation errors (e.g. math). We also take a closer look\n",
       "at the self-discovered reasoning structures, and show the\n",
       "universality of them by transferability study from PaLM\n",
       "2-L to GPT-4, and from GPT-4 to Llama-2-70B. We hope\n",
       "to encourage more future work on structured reasoning for\n",
       "solving challenging problems using LLMs.\n",
       "2. Self-Discovering Reasoning Structures for\n",
       "Problem-Solving\n",
       "We take inspiration from how humans use prior knowledge\n",
       "and skills to devise a reasoning program to solve prob-\n",
       "lems (Newell et al., 1958; Rasmussen, 1983). When we\n",
       "face a new problem, we often first search internally what\n",
       "knowledge and skills from our prior experience might be\n",
       "helpful to solve it. Then we will attempt to apply relevant\n",
       "knowledge and skills to this task. And finally we will con-\n",
       "nect multiple individual skills and knowledge to solve the\n",
       "problem. We design SELF -DISCOVER to enact these steps\n",
       "into two stages as illustrated in Figure 2.\n",
       "Given a task and a set of reasoning module descriptions\n",
       "representing high-level problem-solving heuristics such as\n",
       "“Use critical thinking” and “Let’s think step by step”, Stage 1\n",
       "of SELF -DISCOVER aims to uncover the intrinsic reasoning\n",
       "structure for solving this task via meta-reasoning. Specifi-\n",
       "cally, we uses three meta-prompts to guide LLMs to select,\n",
       "adapt, and implement an actionable reasoning structure with\n",
       "no labels or training required. We format the structure in\n",
       "key-value pairs similar to JSON due to interpretability and\n",
       "findings on following JSON boosts reasoning and generation\n",
       "quality (Zhou et al., 2023; OpenAI, 2023a). The structure of\n",
       "2SELF -DISCOVER : Large Language Models Self-Compose Reasoning Structures\n",
       "Language \n",
       "Model\n",
       "Atomic Reasoning Modules\n",
       "Reasoning Structure\n",
       "{\n",
       "    \"Type and color of each item\": \"\"\n",
       "    \"Number of items of each color\": \"\"\n",
       "    \"Number of items of each type\": \"\"\n",
       "    \"Number of items of each color and type\": \n",
       "    \"Final answer\":\n",
       "}\n",
       "Task: Reasoning \n",
       "colored objects\n",
       "Self-Discover\n",
       "Language \n",
       "Model\n",
       " Answer\n",
       "Stage 1: Discover Reasoning Structure on Task-Level\n",
       "Stage 2: Solve Problems Using Discovered Structure on Instance-Level\n",
       "Reasoning Structure\n",
       "Task Instance\n",
       "Key-Value pairs\n",
       "Fill in the Values based on \n",
       "Keys during decoding\n",
       "Figure 2.Illustration of using SELF -DISCOVER for problem-solving. Given a generative LM, task, and seed reasoning \n",
       "module\n",
       "descriptions, we guide LMs to generate a reasoning structure in key-value format to solve the task. Finally, models\n",
       "can follow the\n",
       "self-discovered structures to solve the every instance from the task by filling in the values in JSON step-by-step.\n",
       "the meta-prompts and full prompts are shown in Appendix.\n",
       "Stage 1 operates on task-level, meaning we only need to run\n",
       "SELF -DISCOVER once for each task. Then, in Stage 2, we\n",
       "can simply use the discovered reasoning structure to solve\n",
       "every instance of the given task by instructing models to\n",
       "follow the provided structure by filling each key and arrive\n",
       "at a final answer.\n",
       "2.1. Stage 1: Self-Discover Task-Specific Structures\n",
       "The first stage consists of three actions: 1) SELECT, where\n",
       "relevant reasoning modules for task-solving are chosen from\n",
       "the set of reasoning module descriptions; 2) ADAPT, where\n",
       "descriptions of selected reasoning modules are rephrased to\n",
       "be more specific to the task at hand; and 3) IMPLEMENT,\n",
       "where the adapted reasoning descriptions are implemented\n",
       "into a structured actionable plan so that the task can be\n",
       "solved by following the structure.\n",
       "SELECT First, not every reasoning module is helpful for\n",
       "every task, so the first stage of SELF -DISCOVER guides\n",
       "model to select modules that are useful based on task exam-\n",
       "ples. For example, “reflective thinking” might help search\n",
       "for first-principle theories on science problems, while “cre-\n",
       "ative thinking” helps on generating a novel continuation to\n",
       "a story. Given raw set of reasoning module descriptions\n",
       "D such as “critical thinking”, and “break the problem into\n",
       "sub-problems” (full set in Appendix A), and a few task ex-\n",
       "amples without labels ti ∈ T, SELF -DISCOVER first selects\n",
       "a subset of reasoning modulesDS that are useful for solving\n",
       "the tasks by using a model M and a meta-prompt pS:\n",
       "DS = M(pS ∥ D ∥ ti). (1)\n",
       "ADAPT Since each reasoning module provides a general\n",
       "description of how to solve problems, the next step of SELF -\n",
       "DISCOVER aims at tailoring each selected module to the\n",
       "task at hand. For example, from “break the problem into sub-\n",
       "problems” to “calculate each arithmetic operation in order”\n",
       "for arithmetic problems. Given selected reasoning module\n",
       "subset DS from the previous step, ADAPT rephrases each\n",
       "of the selected module to be more specific to the task. Sim-\n",
       "ilarly to SELECT, this stage uses a meta-prompt pA and\n",
       "a generative model M to generate the adapted reasoning\n",
       "module descriptions DA:\n",
       "DA = M(pA ∥ DS ∥ ti). (2)\n",
       "IMPLEMENT Finally, given the adapted reasoning mod-\n",
       "ule descriptions DA, SELF -DISCOVER operationalizes the\n",
       "reasoning modules into an implemented reasoning struc-\n",
       "ture DI with specified instruction on what to generate for\n",
       "each step. In addition to a meta prompt pI, IMPLEMENT\n",
       "also provides a demonstration of a human-written reason-\n",
       "ing structure Shuman on another task to better convert the\n",
       "natural language descriptions into a reasoning structure:\n",
       "DI = M(pA ∥ Shuman ∥ DA ∥ ti). (3)\n",
       "2.2. Stage 2: Tackle Tasks Using Discovered Structures\n",
       "After the three stages, we have an implemented reasoning\n",
       "structure DI uniquely adapted for the task we need to solve\n",
       "T. Then we can simply append the reasoning structure to\n",
       "all instances of the task and prompt models to follow the\n",
       "reasoning structure to generate an answer A:\n",
       "A = M(DS ∥ t), ∀t ∈ T. (4)\n",
       "More details of prompts are included in Appendix A.\n",
       "3. Experiment Setup\n",
       "3.1. Tasks\n",
       "We focus on diverse reasoning benchmarks that are still\n",
       "challenging for LLMs: BIG-Bench Hard (BBH) (Suzgun\n",
       "3\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 22.79 seconds| Input tokens: 19,601 | Output tokens: 877]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 22.79 seconds| Input tokens: 19,601 | Output tokens: 877]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m6\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Self-Discover: Large Language Models Self-Compose Reasoning Structures introduces a framework for</span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">LLMs to self-discover and utilize task-specific reasoning structures, enhancing performance on complex </span><span style=\"background-color: #272822\">        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">reasoning tasks compared to traditional prompting methods.\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                   </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mSelf-Discover: Large Language Models Self-Compose Reasoning Structures introduces a framework for\u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mLLMs to self-discover and utilize task-specific reasoning structures, enhancing performance on complex \u001b[0m\u001b[48;2;39;40;34m        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mreasoning tasks compared to traditional prompting methods.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: Self-Discover: Large Language Models Self-Compose Reasoning Structures introduces a framework </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">for LLMs to self-discover and utilize task-specific reasoning structures, enhancing performance on complex </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">reasoning tasks compared to traditional prompting methods.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mOut - Final answer: Self-Discover: Large Language Models Self-Compose Reasoning Structures introduces a framework \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfor LLMs to self-discover and utilize task-specific reasoning structures, enhancing performance on complex \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mreasoning tasks compared to traditional prompting methods.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 5: Duration 32.43 seconds| Input tokens: 33,418 | Output tokens: 1,485]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 5: Duration 32.43 seconds| Input tokens: 33,418 | Output tokens: 1,485]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Self-Discover: Large Language Models Self-Compose Reasoning Structures introduces a framework for LLMs to self-discover and utilize task-specific reasoning structures, enhancing performance on complex reasoning tasks compared to traditional prompting methods.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smolagents import CodeAgent, HfApiModel\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "#model_id = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
    "\n",
    "#model_id = \"meta-llama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "model = HfApiModel(model_id=model_id, token=os.environ[\"HF_API_KEY\"])\n",
    "agent = CodeAgent(tools=[get_hugging_face_top_daily_paper,\n",
    "                         get_paper_id_by_title,\n",
    "                         download_paper_by_id,\n",
    "                         read_pdf_file],\n",
    "                  model=model,\n",
    "                  add_base_tools=True)\n",
    "\n",
    "agent.run(\n",
    "    \"Summarize today's top paper on Hugging Face daily papers by reading it.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and further resources\n",
    "\n",
    "Well that wasn't a complete success...\n",
    "\n",
    "### Further reading:\n",
    "\n",
    "- [Smolagents documentation](https://huggingface.co/docs/smolagents/index)\n",
    "- [Smolagents GitHub repo](https://github.com/huggingface/smolagents)\n",
    "- [About Gwen2.5-Coder Series](https://qwenlm.github.io/blog/qwen2.5-coder-family/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
